{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 100) (2034,)\n",
      "(1353, 100) (1353,)\n"
     ]
    }
   ],
   "source": [
    "# %% 1\n",
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',  categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  categories=categories)\n",
    "\n",
    "# pprint(newsgroups_train.data[0])\n",
    "\n",
    "num_train = len(newsgroups_train.data)\n",
    "num_test  = len(newsgroups_test.data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "X = vectorizer.fit_transform( newsgroups_train.data + newsgroups_test.data )\n",
    "X_train = X[0:num_train, :]\n",
    "X_test = X[num_train:num_train+num_test,:]\n",
    "\n",
    "Y_train = newsgroups_train.target\n",
    "Y_test = newsgroups_test.target\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(input_dim, output_dim, actFunc):\n",
    "    np.random.seed(0)\n",
    "    W = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)\n",
    "    b = np.zeros((1,output_dim))\n",
    "#     print('w:',W.shape)\n",
    "#     print('b:',b.shape)\n",
    "    layer = {'W': W, 'b': b, 'actFunc': actFunc}\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture):\n",
    "    layers = []\n",
    "    for l in nn_architecture:\n",
    "        layer = init_layer(l['input_dim'], l['output_dim'], l['actFunc'])\n",
    "        layers.append(layer)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_scores = np.exp(Z)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1-sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def tanh_backward(dA, Z):\n",
    "    t = np.tanh(Z)\n",
    "    res = (1 - t * t)\n",
    "#     print('res:', res.shape)\n",
    "#     print('dA:', dA.shape)\n",
    "    return res * dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Z, y):\n",
    "    # 计算损失\n",
    "    probs = softmax(Z)\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    #在损失上加上正则项（可选）\n",
    "    # data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_prop(X, layer):\n",
    "    W = layer['W']\n",
    "#     print(W.shape)\n",
    "    Z = X.dot(layer['W']) + layer['b']\n",
    "    if layer['actFunc'] is 'relu':\n",
    "        actFunction = relu\n",
    "    elif layer['actFunc'] is 'sigmoid':\n",
    "        actFunction = sigmoid\n",
    "    else:\n",
    "        actFunction = np.tanh\n",
    "    return actFunction(Z), Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_layers_forward_prop(X, layers):\n",
    "    memory_forward = []\n",
    "    Z_out = X\n",
    "    memo_forward = {\n",
    "        'Z_out': X\n",
    "    }\n",
    "    memory_forward.append(memo_forward)\n",
    "    layers_now = 0\n",
    "    for layer in layers:\n",
    "#         print('forward layers_now:',layers_now)\n",
    "        Z_out, Z_hide = single_layer_forward_prop(Z_out, layer)\n",
    "        memo_forward = {\n",
    "            'Z_out': Z_out,\n",
    "            'Z_hide': Z_hide\n",
    "        }\n",
    "        memory_forward.append(memo_forward)\n",
    "        layers_now += 1\n",
    "\n",
    "    # 返回最终的Z_out => actFunc(Z=X*W + b)\n",
    "    # memory_forward记录每一层的Z_out=actFunc(Z_hide)和Z_hide=W*X+b\n",
    "#     print('Z_out: ',Z_out.shape)\n",
    "    return Z_out, memory_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_prop(memo_forward_now, memo_forward_pre, dA_now, layer):\n",
    "    # 前向神经元个数\n",
    "    # dA_now为由下一层传回的梯度\n",
    "    # memo_forward_pre 记录上一层计算结果， Z_hide=X*w+b和Z_out => X_pre\n",
    "    # memo_forward_now 记录当前层的计算结果，Z_hide => Z_now和Z_out\n",
    "    X_pre = memo_forward_pre['Z_out']\n",
    "    Z_now = memo_forward_now['Z_hide']\n",
    "    back_dim = X_pre.shape[0]\n",
    "\n",
    "    if layer['actFunc'] is 'sigmoid':\n",
    "        actFuncBack = sigmoid_backward\n",
    "    elif layer['actFunc'] is 'relu':\n",
    "        actFuncBack = relu_backward\n",
    "    else:\n",
    "        actFuncBack = tanh_backward\n",
    "\n",
    "    # 计算当前层外层导数\n",
    "    # dZ_now = actFunc'(Z_hide)\n",
    "    dZ_now = actFuncBack(dA_now, Z_now)\n",
    "    # dW_now = actFunc'(Z_hide) * (X=Z_hide*dW)\n",
    "#     print('X_pre',X_pre.shape)\n",
    "#     print('dZ_now',dZ_now.shape)\n",
    "#     print('dA_now',dA_now.shape)\n",
    "#     print('Z_now',Z_now.shape)\n",
    "    dW_now = X_pre.T.dot(dZ_now) / back_dim\n",
    "    # db_now = actFunc'(Z_hide) * (1=Z_hide*db); 维度转换\n",
    "    db_now = np.sum(dZ_now, axis=0, keepdims=True) / back_dim\n",
    "#     print('dW_now:',dW_now.shape)\n",
    "#     print('db_now',db_now.shape)\n",
    "    # dA_pre为向前一层传递的梯度；对上一层的Z_out即本层的X求导结果\n",
    "    # dA_pre = actFunc'(Z_hide) * (W=Z_hide*dX)\n",
    "    W_now = copy.deepcopy(layer['W'])\n",
    "    dA_pre = dZ_now.dot(W_now.T)\n",
    "#     print('dA_pre',dA_pre.shape)\n",
    "    \n",
    "    return dA_pre,dW_now, db_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_layers_backward_prop(Z_out, memory_forward, layers, X, y):\n",
    "#     Z_out, memo_forward = full_layers_forward_prop(X, layers)\n",
    "    # 反向传播\n",
    "    probs = softmax(Z_out)\n",
    "    probs[range(num_examples), y] -= 1\n",
    "    dA_pre = probs\n",
    "#     print('dA_now:', dA_now.shape)\n",
    "#     print('probs:', probs.shape)\n",
    "    memory_backward = []\n",
    "    layers.reverse()\n",
    "    memory_forward.reverse()\n",
    "\n",
    "    length = len(layers)\n",
    "    for idx in range(length):\n",
    "#         print('layer_now:', idx)\n",
    "        dA_pre, dW_now, db_now = single_layer_backward_prop(memory_forward[idx],memory_forward[idx+1],dA_pre,layers[idx])\n",
    "        memo_backward = {\n",
    "            'dW_now': dW_now,\n",
    "            'db_now': db_now\n",
    "        }\n",
    "        memory_backward.append(memo_backward)\n",
    "\n",
    "    return memory_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(layers, memory_backward,learning_rate):\n",
    "#     print('layers: ',len(layers)\n",
    "#     print('memory_backward: ', len(memory_backward))\n",
    "    length = len(layers)\n",
    "#     print(memory_backward)\n",
    "#     print(layers)\n",
    "#     print(memory_backward)\n",
    "    for idx in range(length):\n",
    "        dW = memory_backward[idx]['dW_now']\n",
    "#         print('dW.shape: ', dW.shape)\n",
    "        layers[idx]['W'] -= learning_rate * memory_backward[idx]['dW_now']\n",
    "        layers[idx]['b'] -= learning_rate * memory_backward[idx]['db_now']\n",
    "        \n",
    "#     print(memory_backward)\n",
    "#     print(layers)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, layers):\n",
    "    Z_out, memory_forward = full_layers_forward_prop(X,layers)\n",
    "    probs = softmax(Z_out)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(X, layers):\n",
    "    acc = np.mean(Y_test==predict(X, layers))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, nn_architcture, epochs,base_lr):\n",
    "    layers = init_layers(nn_architcture)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    best_acc = 0\n",
    "    lr=base_lr\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Z_out, memory_forward = full_layers_forward_prop(X,layers)\n",
    "#         print(Z_out.shape)\n",
    "        cost = loss(Z_out, y)\n",
    "        acc = get_acc(X_test, layers)\n",
    "        cost_history.append(cost)\n",
    "        accuracy_history.append(acc)\n",
    "        if best_acc < acc :\n",
    "            best_acc = acc\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print('||best_acc => ', best_acc, '||cost => ', cost, '||acc => ', acc)\n",
    "            print('cost: ', cost)\n",
    "            print('acc: ', acc)\n",
    "            print('learning_rate',lr)\n",
    "\n",
    "        #lr=base_lr/(1+1e-6*(i+1))\n",
    "        memory_backward = full_layers_backward_prop(Z_out, memory_forward, layers, X, y)\n",
    "        layers = update(layers, memory_backward,lr)\n",
    "        \n",
    "        layers.reverse()\n",
    "        memory_forward.reverse()\n",
    "\n",
    "    return layers, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = X_train.shape[0] # 训练样本的数量\n",
    "nn_input_dim = X_train.shape[1] # 输入层的维度\n",
    "nn_output_dim = 4 # 输出层的维度\n",
    "\n",
    "# 梯度下降的参数（我直接手动赋值）\n",
    "epsilon = 0.05 # 初始的学习率\n",
    "reg_lambda = 0.01 # 正则化的强度\n",
    "epochs = 2000\n",
    "\n",
    "print('样本数量：', num_examples)\n",
    "print('输入样本维度：',nn_input_dim)\n",
    "print('输出数量：',nn_output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn_architcture = [\n",
    "     {'input_dim': nn_input_dim, 'output_dim': 20, 'actFunc': 'relu'},\n",
    "    {'input_dim': 20, 'output_dim': 20, 'actFunc': 'relu'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 20, 'output_dim': nn_output_dim, 'actFunc': 'relu'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 1.\")\n",
    "print(\"Decision Boundary for hidden layer size 1. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||best_acc =>  0.18551367331855137 ||cost =>  1.3935356280445361 ||acc =>  0.18551367331855137\n",
      "cost:  1.3935356280445361\n",
      "acc:  0.18551367331855137\n",
      "learning_rate 0.01\n",
      "||best_acc =>  0.22838137472283815 ||cost =>  1.37645351472104 ||acc =>  0.2246858832224686\n",
      "cost:  1.37645351472104\n",
      "acc:  0.2246858832224686\n",
      "learning_rate 0.009949628981268435\n",
      "||best_acc =>  0.25942350332594233 ||cost =>  1.3661297178539016 ||acc =>  0.25942350332594233\n",
      "cost:  1.3661297178539016\n",
      "acc:  0.25942350332594233\n",
      "learning_rate 0.009801019748273843\n",
      "||best_acc =>  0.28972653362897266 ||cost =>  1.357654499160218 ||acc =>  0.28898743532889876\n",
      "cost:  1.357654499160218\n",
      "acc:  0.28898743532889876\n",
      "learning_rate 0.009558584151996714\n",
      "||best_acc =>  0.3148558758314856 ||cost =>  1.3497588007067556 ||acc =>  0.3148558758314856\n",
      "cost:  1.3497588007067556\n",
      "acc:  0.3148558758314856\n",
      "learning_rate 0.009229416212063095\n",
      "||best_acc =>  0.3451589061345159 ||cost =>  1.342023928950117 ||acc =>  0.3451589061345159\n",
      "cost:  1.342023928950117\n",
      "acc:  0.3451589061345159\n",
      "learning_rate 0.008822947374504043\n",
      "||best_acc =>  0.36363636363636365 ||cost =>  1.3343010579585262 ||acc =>  0.36363636363636365\n",
      "cost:  1.3343010579585262\n",
      "acc:  0.36363636363636365\n",
      "learning_rate 0.008350497953312618\n",
      "||best_acc =>  0.3924611973392461 ||cost =>  1.3265577465878535 ||acc =>  0.3924611973392461\n",
      "cost:  1.3265577465878535\n",
      "acc:  0.3924611973392461\n",
      "learning_rate 0.00782475450023322\n",
      "||best_acc =>  0.41832963784183297 ||cost =>  1.3188273253040368 ||acc =>  0.41832963784183297\n",
      "cost:  1.3188273253040368\n",
      "acc:  0.41832963784183297\n",
      "learning_rate 0.007259206694903311\n",
      "||best_acc =>  0.43532889874353287 ||cost =>  1.3111849738772892 ||acc =>  0.43532889874353287\n",
      "cost:  1.3111849738772892\n",
      "acc:  0.43532889874353287\n",
      "learning_rate 0.006667578435118034\n",
      "||best_acc =>  0.4604582409460458 ||cost =>  1.3037302068184335 ||acc =>  0.4604582409460458\n",
      "cost:  1.3037302068184335\n",
      "acc:  0.4604582409460458\n",
      "learning_rate 0.00606328617553024\n",
      "||best_acc =>  0.4722838137472284 ||cost =>  1.2965710783518376 ||acc =>  0.4708056171470806\n",
      "cost:  1.2965710783518376\n",
      "acc:  0.4708056171470806\n",
      "learning_rate 0.005458953511189144\n",
      "||best_acc =>  0.4804138950480414 ||cost =>  1.289810225179722 ||acc =>  0.4804138950480414\n",
      "cost:  1.289810225179722\n",
      "acc:  0.4804138950480414\n",
      "learning_rate 0.004866005040055407\n",
      "||best_acc =>  0.49223946784922396 ||cost =>  1.283534001842324 ||acc =>  0.49150036954915005\n",
      "cost:  1.283534001842324\n",
      "acc:  0.49150036954915005\n",
      "learning_rate 0.004294355320114214\n",
      "||best_acc =>  0.49889135254988914 ||cost =>  1.277805625145415 ||acc =>  0.49815225424981524\n",
      "cost:  1.277805625145415\n",
      "acc:  0.49815225424981524\n",
      "learning_rate 0.00375220098118162\n",
      "||best_acc =>  0.50849963045085 ||cost =>  1.2726624234027093 ||acc =>  0.5077605321507761\n",
      "cost:  1.2726624234027093\n",
      "acc:  0.5077605321507761\n",
      "learning_rate 0.0032459164635409918\n",
      "||best_acc =>  0.5129342202512934 ||cost =>  1.2681165467858422 ||acc =>  0.5121951219512195\n",
      "cost:  1.2681165467858422\n",
      "acc:  0.5121951219512195\n",
      "learning_rate 0.002780047054121764\n",
      "||best_acc =>  0.516629711751663 ||cost =>  1.2641580928005522 ||acc =>  0.516629711751663\n",
      "cost:  1.2641580928005522\n",
      "acc:  0.516629711751663\n",
      "learning_rate 0.002357387355038093\n",
      "||best_acc =>  0.5203252032520326 ||cost =>  1.2607595497739241 ||acc =>  0.5203252032520326\n",
      "cost:  1.2607595497739241\n",
      "acc:  0.5203252032520326\n",
      "learning_rate 0.0019791293550243875\n",
      "||best_acc =>  0.5240206947524021 ||cost =>  1.2578806438720227 ||acc =>  0.5240206947524021\n",
      "cost:  1.2578806438720227\n",
      "acc:  0.5240206947524021\n",
      "learning_rate 0.0016450620048178398\n",
      "||best_acc =>  0.5262379896526238 ||cost =>  1.2554729510185012 ||acc =>  0.5262379896526238\n",
      "cost:  1.2554729510185012\n",
      "acc:  0.5262379896526238\n",
      "learning_rate 0.0013538035744896269\n",
      "||best_acc =>  0.5284552845528455 ||cost =>  1.2534839023093347 ||acc =>  0.5284552845528455\n",
      "cost:  1.2534839023093347\n",
      "acc:  0.5284552845528455\n",
      "learning_rate 0.001103048902832196\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2518600206487902 ||acc =>  0.5291943828529194\n",
      "cost:  1.2518600206487902\n",
      "acc:  0.5291943828529194\n",
      "learning_rate 0.0008898156420400727\n",
      "||best_acc =>  0.5306725794530672 ||cost =>  1.2505493672087609 ||acc =>  0.5306725794530672\n",
      "cost:  1.2505493672087609\n",
      "acc:  0.5306725794530672\n",
      "learning_rate 0.0007106764032672958\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.249503259122773 ||acc =>  0.532150776053215\n",
      "cost:  1.249503259122773\n",
      "acc:  0.532150776053215\n",
      "learning_rate 0.0005619669548978453\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2486773608460517 ||acc =>  0.532150776053215\n",
      "cost:  1.2486773608460517\n",
      "acc:  0.532150776053215\n",
      "learning_rate 0.00043996397493724544\n",
      "||best_acc =>  0.5328898743532889 ||cost =>  1.2480322657528857 ||acc =>  0.5328898743532889\n",
      "cost:  1.2480322657528857\n",
      "acc:  0.5328898743532889\n",
      "learning_rate 0.00034102902480758195\n",
      "||best_acc =>  0.5328898743532889 ||cost =>  1.2475336828565653 ||acc =>  0.532150776053215\n",
      "cost:  1.2475336828565653\n",
      "acc:  0.532150776053215\n",
      "learning_rate 0.000261718178412039\n",
      "||best_acc =>  0.5328898743532889 ||cost =>  1.2471523333139118 ||acc =>  0.532150776053215\n",
      "cost:  1.2471523333139118\n",
      "acc:  0.532150776053215\n",
      "learning_rate 0.000198858970331125\n",
      "||best_acc =>  0.5336289726533628 ||cost =>  1.2468636471046322 ||acc =>  0.5336289726533628\n",
      "cost:  1.2468636471046322\n",
      "acc:  0.5336289726533628\n",
      "learning_rate 0.00014959795867428786\n",
      "||best_acc =>  0.5343680709534369 ||cost =>  1.2466473346324143 ||acc =>  0.5343680709534369\n",
      "cost:  1.2466473346324143\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 0.00011142323515659665\n",
      "||best_acc =>  0.5343680709534369 ||cost =>  1.2464868925652293 ||acc =>  0.5343680709534369\n",
      "cost:  1.2464868925652293\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 8.216671185085448e-05\n",
      "||best_acc =>  0.5343680709534369 ||cost =>  1.2463690889526415 ||acc =>  0.5343680709534369\n",
      "cost:  1.2463690889526415\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 5.999105925699594e-05\n",
      "||best_acc =>  0.5343680709534369 ||cost =>  1.2462834600590558 ||acc =>  0.5343680709534369\n",
      "cost:  1.2462834600590558\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 4.336586997519258e-05\n",
      "||best_acc =>  0.5343680709534369 ||cost =>  1.2462218406920642 ||acc =>  0.5343680709534369\n",
      "cost:  1.2462218406920642\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 3.103708590322913e-05\n",
      "||best_acc =>  0.5343680709534369 ||cost =>  1.2461779411472012 ||acc =>  0.5343680709534369\n",
      "cost:  1.2461779411472012\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 2.1993056325353996e-05\n",
      "||best_acc =>  0.5343680709534369 ||cost =>  1.2461469771560445 ||acc =>  0.5343680709534369\n",
      "cost:  1.2461469771560445\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 1.542987612083013e-05\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2461253542382436 ||acc =>  0.5351071692535108\n",
      "cost:  1.2461253542382436\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.071795496881344e-05\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2461104043854434 ||acc =>  0.5351071692535108\n",
      "cost:  1.2461104043854434\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 7.371137011082158e-06\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2461001707851789 ||acc =>  0.5351071692535108\n",
      "cost:  1.2461001707851789\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 5.019154189007542e-06\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460932350632157 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460932350632157\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 3.3837675418258724e-06\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246088581038156 ||acc =>  0.5351071692535108\n",
      "cost:  1.246088581038156\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.2586288067128286e-06\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246085489025864 ||acc =>  0.5351071692535108\n",
      "cost:  1.246085489025864\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.4926706593616446e-06\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246083455121929 ||acc =>  0.5351071692535108\n",
      "cost:  1.246083455121929\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 9.766936203195799e-07\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460821304833094 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460821304833094\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 6.327444692605093e-07\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460812763159417 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460812763159417\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 4.058583141664043e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460807309753403 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460807309753403\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.5774903588173645e-07\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460803862504852 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460803862504852\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.6206774717369139e-07\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460801704969977 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460801704969977\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.0089589908624819e-07\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246080036799271 ||acc =>  0.5351071692535108\n",
      "cost:  1.246080036799271\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 6.21910999424082e-08\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460799547694674 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460799547694674\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 3.795431829180078e-08\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460799049382802 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460799049382802\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.2933630799834968e-08\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246079874966457 ||acc =>  0.5351071692535108\n",
      "cost:  1.246079874966457\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.3720297613719517e-08\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798571177036 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798571177036\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 8.127065707862755e-09\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798465935818 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798465935818\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 4.766328419934127e-09\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798404496338 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798404496338\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.7676714875823294e-09\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246079836898286 ||acc =>  0.5351071692535108\n",
      "cost:  1.246079836898286\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.591204244149406e-09\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798348658189 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798348658189\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 9.057713084141972e-10\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798337141203 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798337141203\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 5.104966102332723e-10\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798330679614 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798330679614\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.848716734404242e-10\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798327090181 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798327090181\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.5739401039340634e-10\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798325115947 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798325115947\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 8.610137667257476e-11\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798324040827 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798324040827\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 4.663536478594634e-11\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798323461124 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798323461124\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.500946944505235e-11\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246079832315164 ||acc =>  0.5351071692535108\n",
      "cost:  1.246079832315164\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.3279380822006474e-11\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246079832298805 ||acc =>  0.5351071692535108\n",
      "cost:  1.246079832298805\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 6.981292720929258e-12\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322902429 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322902429\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 3.6339502271032315e-12\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322858064 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322858064\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.872870004032548e-12\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.24607983228353 ||acc =>  0.5351071692535108\n",
      "cost:  1.24607983228353\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 9.55701471741787e-13\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322823732 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322823732\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 4.82862263035395e-13\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322817919 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322817919\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.41552254013679e-13\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322815019 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322815019\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.1964269026767857e-13\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246079832281359 ||acc =>  0.5351071692535108\n",
      "cost:  1.246079832281359\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 5.867443554604624e-14\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812894 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812894\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.84904804938641e-14\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812556 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812556\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.3697432159164425e-14\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812396 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812396\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 6.5202988506085e-15\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812323 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812323\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 3.0731604257334284e-15\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812296 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812296\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.4341440554569624e-15\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246079832281229 ||acc =>  0.5351071692535108\n",
      "cost:  1.246079832281229\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 6.626597078359254e-16\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.246079832281229 ||acc =>  0.5351071692535108\n",
      "cost:  1.246079832281229\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 3.031649729421591e-16\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.373278363706493e-16\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 6.159275896696616e-17\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.7352226942408395e-17\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.2026745031087255e-17\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 5.235960044262831e-18\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.2570323183379067e-18\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 9.6332513709762e-19\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 4.0710093169908245e-19\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.7034359742733084e-19\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 7.057395804965852e-20\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.89506623191295e-20\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.1758944151528896e-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 4.729055091374465e-21\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.883115973193672e-21\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 7.424664529129591e-22\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 2.8985053395457594e-22\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.1203901988019215e-22\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 4.288079715289919e-23\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 1.6250067854645267e-23\n",
      "||best_acc =>  0.5351071692535108 ||cost =>  1.2460798322812288 ||acc =>  0.5351071692535108\n",
      "cost:  1.2460798322812288\n",
      "acc:  0.5351071692535108\n",
      "learning_rate 6.0974284442187356e-24\n",
      "Decision Boundary for hidden layer size 3. acc:  0.5351071692535108\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 20, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 20, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 20, 'output_dim': 8, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 20, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 3\")\n",
    "print(\"Decision Boundary for hidden layer size 3. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||best_acc =>  0.29637841832963785 ||cost =>  1.373042324108176 ||acc =>  0.29637841832963785\n",
      "cost:  1.373042324108176\n",
      "acc:  0.29637841832963785\n",
      "learning_rate 0.01\n",
      "||best_acc =>  0.31263858093126384 ||cost =>  1.3580467054868781 ||acc =>  0.31263858093126384\n",
      "cost:  1.3580467054868781\n",
      "acc:  0.31263858093126384\n",
      "learning_rate 0.009949628981268435\n",
      "||best_acc =>  0.3340724316334072 ||cost =>  1.3479520935995346 ||acc =>  0.3340724316334072\n",
      "cost:  1.3479520935995346\n",
      "acc:  0.3340724316334072\n",
      "learning_rate 0.009801019748273843\n",
      "||best_acc =>  0.35550628233555065 ||cost =>  1.3399167305924713 ||acc =>  0.35476718403547675\n",
      "cost:  1.3399167305924713\n",
      "acc:  0.35476718403547675\n",
      "learning_rate 0.009558584151996714\n",
      "||best_acc =>  0.3821138211382114 ||cost =>  1.3328393361808368 ||acc =>  0.3821138211382114\n",
      "cost:  1.3328393361808368\n",
      "acc:  0.3821138211382114\n",
      "learning_rate 0.009229416212063095\n",
      "||best_acc =>  0.41611234294161126 ||cost =>  1.3262966869486423 ||acc =>  0.41611234294161126\n",
      "cost:  1.3262966869486423\n",
      "acc:  0.41611234294161126\n",
      "learning_rate 0.008822947374504043\n",
      "||best_acc =>  0.43237250554323725 ||cost =>  1.3201356041594283 ||acc =>  0.43237250554323725\n",
      "cost:  1.3201356041594283\n",
      "acc:  0.43237250554323725\n",
      "learning_rate 0.008350497953312618\n",
      "||best_acc =>  0.4449371766444937 ||cost =>  1.3143138083432033 ||acc =>  0.4441980783444198\n",
      "cost:  1.3143138083432033\n",
      "acc:  0.4441980783444198\n",
      "learning_rate 0.00782475450023322\n",
      "||best_acc =>  0.4597191426459719 ||cost =>  1.3088342383507 ||acc =>  0.4597191426459719\n",
      "cost:  1.3088342383507\n",
      "acc:  0.4597191426459719\n",
      "learning_rate 0.007259206694903311\n",
      "||best_acc =>  0.4715447154471545 ||cost =>  1.303715986746138 ||acc =>  0.4715447154471545\n",
      "cost:  1.303715986746138\n",
      "acc:  0.4715447154471545\n",
      "learning_rate 0.006667578435118034\n",
      "||best_acc =>  0.48337028824833705 ||cost =>  1.2989805338632125 ||acc =>  0.48337028824833705\n",
      "cost:  1.2989805338632125\n",
      "acc:  0.48337028824833705\n",
      "learning_rate 0.00606328617553024\n",
      "||best_acc =>  0.49223946784922396 ||cost =>  1.2946449776894409 ||acc =>  0.49223946784922396\n",
      "cost:  1.2946449776894409\n",
      "acc:  0.49223946784922396\n",
      "learning_rate 0.005458953511189144\n",
      "||best_acc =>  0.5011086474501109 ||cost =>  1.290718846767303 ||acc =>  0.5011086474501109\n",
      "cost:  1.290718846767303\n",
      "acc:  0.5011086474501109\n",
      "learning_rate 0.004866005040055407\n",
      "||best_acc =>  0.5062823355506282 ||cost =>  1.2872029617816814 ||acc =>  0.5055432372505543\n",
      "cost:  1.2872029617816814\n",
      "acc:  0.5055432372505543\n",
      "learning_rate 0.004294355320114214\n",
      "||best_acc =>  0.5114560236511456 ||cost =>  1.2840895535762313 ||acc =>  0.5114560236511456\n",
      "cost:  1.2840895535762313\n",
      "acc:  0.5114560236511456\n",
      "learning_rate 0.00375220098118162\n",
      "||best_acc =>  0.515890613451589 ||cost =>  1.2813631570982238 ||acc =>  0.515890613451589\n",
      "cost:  1.2813631570982238\n",
      "acc:  0.515890613451589\n",
      "learning_rate 0.0032459164635409918\n",
      "||best_acc =>  0.5225424981522543 ||cost =>  1.2790019510109203 ||acc =>  0.5225424981522543\n",
      "cost:  1.2790019510109203\n",
      "acc:  0.5225424981522543\n",
      "learning_rate 0.002780047054121764\n",
      "||best_acc =>  0.5240206947524021 ||cost =>  1.2769793023663565 ||acc =>  0.5225424981522543\n",
      "cost:  1.2769793023663565\n",
      "acc:  0.5225424981522543\n",
      "learning_rate 0.002357387355038093\n",
      "||best_acc =>  0.5254988913525499 ||cost =>  1.2752653409075585 ||acc =>  0.5254988913525499\n",
      "cost:  1.2752653409075585\n",
      "acc:  0.5254988913525499\n",
      "learning_rate 0.0019791293550243875\n",
      "||best_acc =>  0.5284552845528455 ||cost =>  1.2738284400575686 ||acc =>  0.5284552845528455\n",
      "cost:  1.2738284400575686\n",
      "acc:  0.5284552845528455\n",
      "learning_rate 0.0016450620048178398\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2726365248648233 ||acc =>  0.5291943828529194\n",
      "cost:  1.2726365248648233\n",
      "acc:  0.5291943828529194\n",
      "learning_rate 0.0013538035744896269\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2716581621347365 ||acc =>  0.5291943828529194\n",
      "cost:  1.2716581621347365\n",
      "acc:  0.5291943828529194\n",
      "learning_rate 0.001103048902832196\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2708634153141103 ||acc =>  0.5291943828529194\n",
      "cost:  1.2708634153141103\n",
      "acc:  0.5291943828529194\n",
      "learning_rate 0.0008898156420400727\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2702244671333167 ||acc =>  0.5291943828529194\n",
      "cost:  1.2702244671333167\n",
      "acc:  0.5291943828529194\n",
      "learning_rate 0.0007106764032672958\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2697160273957515 ||acc =>  0.5291943828529194\n",
      "cost:  1.2697160273957515\n",
      "acc:  0.5291943828529194\n",
      "learning_rate 0.0005619669548978453\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2693155525315452 ||acc =>  0.5284552845528455\n",
      "cost:  1.2693155525315452\n",
      "acc:  0.5284552845528455\n",
      "learning_rate 0.00043996397493724544\n",
      "||best_acc =>  0.5291943828529194 ||cost =>  1.2690033084995245 ||acc =>  0.5291943828529194\n",
      "cost:  1.2690033084995245\n",
      "acc:  0.5291943828529194\n",
      "learning_rate 0.00034102902480758195\n",
      "||best_acc =>  0.5299334811529933 ||cost =>  1.268762310196252 ||acc =>  0.5299334811529933\n",
      "cost:  1.268762310196252\n",
      "acc:  0.5299334811529933\n",
      "learning_rate 0.000261718178412039\n",
      "||best_acc =>  0.5306725794530672 ||cost =>  1.2685781695353522 ||acc =>  0.5306725794530672\n",
      "cost:  1.2685781695353522\n",
      "acc:  0.5306725794530672\n",
      "learning_rate 0.000198858970331125\n",
      "||best_acc =>  0.5306725794530672 ||cost =>  1.2684388815516443 ||acc =>  0.5306725794530672\n",
      "cost:  1.2684388815516443\n",
      "acc:  0.5306725794530672\n",
      "learning_rate 0.00014959795867428786\n",
      "||best_acc =>  0.5306725794530672 ||cost =>  1.2683345739365757 ||acc =>  0.5306725794530672\n",
      "cost:  1.2683345739365757\n",
      "acc:  0.5306725794530672\n",
      "learning_rate 0.00011142323515659665\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2682572408968154 ||acc =>  0.532150776053215\n",
      "cost:  1.2682572408968154\n",
      "acc:  0.532150776053215\n",
      "learning_rate 8.216671185085448e-05\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268200477606798 ||acc =>  0.532150776053215\n",
      "cost:  1.268200477606798\n",
      "acc:  0.532150776053215\n",
      "learning_rate 5.999105925699594e-05\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2681592271425504 ||acc =>  0.532150776053215\n",
      "cost:  1.2681592271425504\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.336586997519258e-05\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2681295478714187 ||acc =>  0.532150776053215\n",
      "cost:  1.2681295478714187\n",
      "acc:  0.532150776053215\n",
      "learning_rate 3.103708590322913e-05\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2681084059645717 ||acc =>  0.532150776053215\n",
      "cost:  1.2681084059645717\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.1993056325353996e-05\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268093495048441 ||acc =>  0.532150776053215\n",
      "cost:  1.268093495048441\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.542987612083013e-05\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268083083006618 ||acc =>  0.532150776053215\n",
      "cost:  1.268083083006618\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.071795496881344e-05\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.26807588452995 ||acc =>  0.532150776053215\n",
      "cost:  1.26807588452995\n",
      "acc:  0.532150776053215\n",
      "learning_rate 7.371137011082158e-06\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680709571074902 ||acc =>  0.532150776053215\n",
      "cost:  1.2680709571074902\n",
      "acc:  0.532150776053215\n",
      "learning_rate 5.019154189007542e-06\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680676176600099 ||acc =>  0.532150776053215\n",
      "cost:  1.2680676176600099\n",
      "acc:  0.532150776053215\n",
      "learning_rate 3.3837675418258724e-06\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680653768450967 ||acc =>  0.532150776053215\n",
      "cost:  1.2680653768450967\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.2586288067128286e-06\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268063888119386 ||acc =>  0.532150776053215\n",
      "cost:  1.268063888119386\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.4926706593616446e-06\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680629088517432 ||acc =>  0.532150776053215\n",
      "cost:  1.2680629088517432\n",
      "acc:  0.532150776053215\n",
      "learning_rate 9.766936203195799e-07\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680622710778513 ||acc =>  0.532150776053215\n",
      "cost:  1.2680622710778513\n",
      "acc:  0.532150776053215\n",
      "learning_rate 6.327444692605093e-07\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061859822811 ||acc =>  0.532150776053215\n",
      "cost:  1.268061859822811\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.058583141664043e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680615972586744 ||acc =>  0.532150776053215\n",
      "cost:  1.2680615972586744\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.5774903588173645e-07\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680614312847989 ||acc =>  0.532150776053215\n",
      "cost:  1.2680614312847989\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.6206774717369139e-07\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680613274065338 ||acc =>  0.532150776053215\n",
      "cost:  1.2680613274065338\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.0089589908624819e-07\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061263035466 ||acc =>  0.532150776053215\n",
      "cost:  1.268061263035466\n",
      "acc:  0.532150776053215\n",
      "learning_rate 6.21910999424082e-08\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061223540817 ||acc =>  0.532150776053215\n",
      "cost:  1.268061223540817\n",
      "acc:  0.532150776053215\n",
      "learning_rate 3.795431829180078e-08\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061199548745 ||acc =>  0.532150776053215\n",
      "cost:  1.268061199548745\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.2933630799834968e-08\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611851183026 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611851183026\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.3720297613719517e-08\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061176524718 ||acc =>  0.532150776053215\n",
      "cost:  1.268061176524718\n",
      "acc:  0.532150776053215\n",
      "learning_rate 8.127065707862755e-09\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611714577017 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611714577017\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.766328419934127e-09\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611684995944 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611684995944\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.7676714875823294e-09\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611667897377 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611667897377\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.591204244149406e-09\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611658111718 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611658111718\n",
      "acc:  0.532150776053215\n",
      "learning_rate 9.057713084141972e-10\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611652566671 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611652566671\n",
      "acc:  0.532150776053215\n",
      "learning_rate 5.104966102332723e-10\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164945563 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164945563\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.848716734404242e-10\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164772744 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164772744\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.5739401039340634e-10\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611646776911 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611646776911\n",
      "acc:  0.532150776053215\n",
      "learning_rate 8.610137667257476e-11\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611646259277 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611646259277\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.663536478594634e-11\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645980169 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645980169\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.500946944505235e-11\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645831161 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645831161\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.3279380822006474e-11\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645752398 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645752398\n",
      "acc:  0.532150776053215\n",
      "learning_rate 6.981292720929258e-12\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645711175 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645711175\n",
      "acc:  0.532150776053215\n",
      "learning_rate 3.6339502271032315e-12\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645689814 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645689814\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.872870004032548e-12\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645678852 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645678852\n",
      "acc:  0.532150776053215\n",
      "learning_rate 9.55701471741787e-13\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645673285 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645673285\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.82862263035395e-13\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645670483 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645670483\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.41552254013679e-13\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645669089 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645669089\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.1964269026767857e-13\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.26806116456684 ||acc =>  0.532150776053215\n",
      "cost:  1.26806116456684\n",
      "acc:  0.532150776053215\n",
      "learning_rate 5.867443554604624e-14\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645668065 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645668065\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.84904804938641e-14\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645667903 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645667903\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.3697432159164425e-14\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645667825 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645667825\n",
      "acc:  0.532150776053215\n",
      "learning_rate 6.5202988506085e-15\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566779 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566779\n",
      "acc:  0.532150776053215\n",
      "learning_rate 3.0731604257334284e-15\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645667774 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645667774\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.4341440554569624e-15\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645667774 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645667774\n",
      "acc:  0.532150776053215\n",
      "learning_rate 6.626597078359254e-16\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645667772 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645667772\n",
      "acc:  0.532150776053215\n",
      "learning_rate 3.031649729421591e-16\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.2680611645667772 ||acc =>  0.532150776053215\n",
      "cost:  1.2680611645667772\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.373278363706493e-16\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 6.159275896696616e-17\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.7352226942408395e-17\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.2026745031087255e-17\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 5.235960044262831e-18\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.2570323183379067e-18\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 9.6332513709762e-19\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.0710093169908245e-19\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.7034359742733084e-19\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 7.057395804965852e-20\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.89506623191295e-20\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.1758944151528896e-20\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.729055091374465e-21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.883115973193672e-21\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 7.424664529129591e-22\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 2.8985053395457594e-22\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.1203901988019215e-22\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 4.288079715289919e-23\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 1.6250067854645267e-23\n",
      "||best_acc =>  0.532150776053215 ||cost =>  1.268061164566777 ||acc =>  0.532150776053215\n",
      "cost:  1.268061164566777\n",
      "acc:  0.532150776053215\n",
      "learning_rate 6.0974284442187356e-24\n",
      "Decision Boundary for hidden layer size 2, acc:  0.532150776053215\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 8, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 8, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 2.\")\n",
    "print(\"Decision Boundary for hidden layer size 2, acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'base_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-43e5bd47bf9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_history\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_architcture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# plot_decision_boundary(lambda x: predict(x, layers))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'base_lr'"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 4\")\n",
    "print(\"Decision Boundary for hidden layer size 4. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||best_acc =>  0.2564671101256467 ||cost =>  1.3928003874755799 ||acc =>  0.2564671101256467\n",
      "cost:  1.3928003874755799\n",
      "acc:  0.2564671101256467\n",
      "learning_rate 0.05\n",
      "||best_acc =>  0.5824094604582409 ||cost =>  0.9812202225650672 ||acc =>  0.5691056910569106\n",
      "cost:  0.9812202225650672\n",
      "acc:  0.5691056910569106\n",
      "learning_rate 0.05\n",
      "||best_acc =>  0.6452328159645233 ||cost =>  0.789289300993758 ||acc =>  0.6415373244641537\n",
      "cost:  0.789289300993758\n",
      "acc:  0.6415373244641537\n",
      "learning_rate 0.05\n",
      "||best_acc =>  0.6548410938654841 ||cost =>  0.7579981813165525 ||acc =>  0.6452328159645233\n",
      "cost:  0.7579981813165525\n",
      "acc:  0.6452328159645233\n",
      "learning_rate 0.05\n",
      "||best_acc =>  0.6548410938654841 ||cost =>  0.7495181534481822 ||acc =>  0.6444937176644494\n",
      "cost:  0.7495181534481822\n",
      "acc:  0.6444937176644494\n",
      "learning_rate 0.05\n",
      "Decision Boundary for hidden layer size 4. acc:  0.6496674057649667\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 4\")\n",
    "print(\"Decision Boundary for hidden layer size 4. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
