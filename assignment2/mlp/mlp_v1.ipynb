{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 20) (2034,)\n",
      "(1353, 20) (1353,)\n"
     ]
    }
   ],
   "source": [
    "# %% 1\n",
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',  categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  categories=categories)\n",
    "\n",
    "# pprint(newsgroups_train.data[0])\n",
    "\n",
    "num_train = len(newsgroups_train.data)\n",
    "num_test  = len(newsgroups_test.data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=20)\n",
    "\n",
    "X = vectorizer.fit_transform( newsgroups_train.data + newsgroups_test.data )\n",
    "X_train = X[0:num_train, :]\n",
    "X_test = X[num_train:num_train+num_test,:]\n",
    "\n",
    "Y_train = newsgroups_train.target\n",
    "Y_test = newsgroups_test.target\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate a dataset and plot it\n",
    "# np.random.seed(0)\n",
    "# X, y = sklearn.datasets.make_moons(1000, noise=0.20)\n",
    "# print('输入：',X.shape)\n",
    "# print('输出',y.shape)\n",
    "# plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the logistic regression classifier\n",
    "# clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to plot a decision boundary.\n",
    "# # If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
    "# def plot_decision_boundary(pred_func):\n",
    "#     # Set min and max values and give it some padding\n",
    "#     x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "#     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "#     h = 0.01\n",
    "#     # Generate a grid of points with distance h between them\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "#     # Predict the function value for the whole gid\n",
    "#     Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     # Plot the contour and training examples\n",
    "#     plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "#     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the decision boundary\n",
    "# plot_decision_boundary(lambda x: clf.predict(x))\n",
    "# plt.title(\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(input_dim, output_dim, actFunc):\n",
    "    np.random.seed(0)\n",
    "    W = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)\n",
    "    b = np.zeros((1,output_dim))\n",
    "    print('w:',W.shape)\n",
    "    print('b:',b.shape)\n",
    "    layer = {'W': W, 'b': b, 'actFunc': actFunc}\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture):\n",
    "    layers = []\n",
    "    for l in nn_architecture:\n",
    "        layer = init_layer(l['input_dim'], l['output_dim'], l['actFunc'])\n",
    "        layers.append(layer)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_scores = np.exp(Z)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1-sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def tanh_backward(dA, Z):\n",
    "    t = np.tanh(Z)\n",
    "    res = (1 - t * t)\n",
    "#     print('res:', res.shape)\n",
    "#     print('dA:', dA.shape)\n",
    "    return res * dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Z, y):\n",
    "    # 计算损失\n",
    "    probs = softmax(Z)\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    #在损失上加上正则项（可选）\n",
    "    # data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_prop(X, layer):\n",
    "    W = layer['W']\n",
    "#     print(W.shape)\n",
    "    Z = X.dot(layer['W']) + layer['b']\n",
    "    if layer['actFunc'] is 'relu':\n",
    "        actFunction = relu\n",
    "    elif layer['actFunc'] is 'sigmoid':\n",
    "        actFunction = sigmoid\n",
    "    else:\n",
    "        actFunction = np.tanh\n",
    "    return actFunction(Z), Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_layers_forward_prop(X, layers):\n",
    "    memory_forward = []\n",
    "    Z_out = X\n",
    "    memo_forward = {\n",
    "        'Z_out': X\n",
    "    }\n",
    "    memory_forward.append(memo_forward)\n",
    "    layers_now = 0\n",
    "    for layer in layers:\n",
    "#         print('forward layers_now:',layers_now)\n",
    "        Z_out, Z_hide = single_layer_forward_prop(Z_out, layer)\n",
    "        memo_forward = {\n",
    "            'Z_out': Z_out,\n",
    "            'Z_hide': Z_hide\n",
    "        }\n",
    "        memory_forward.append(memo_forward)\n",
    "        layers_now += 1\n",
    "\n",
    "    # 返回最终的Z_out => actFunc(Z=X*W + b)\n",
    "    # memory_forward记录每一层的Z_out=actFunc(Z_hide)和Z_hide=W*X+b\n",
    "#     print('Z_out: ',Z_out.shape)\n",
    "    return Z_out, memory_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_prop(memo_forward_now, memo_forward_pre, dA_now, layer):\n",
    "    # 前向神经元个数\n",
    "    # dA_now为由下一层传回的梯度\n",
    "    # memo_forward_pre 记录上一层计算结果， Z_hide=X*w+b和Z_out => X_pre\n",
    "    # memo_forward_now 记录当前层的计算结果，Z_hide => Z_now和Z_out\n",
    "    X_pre = memo_forward_pre['Z_out']\n",
    "    Z_now = memo_forward_now['Z_hide']\n",
    "    back_dim = X_pre.shape[0]\n",
    "\n",
    "    if layer['actFunc'] is 'sigmoid':\n",
    "        actFuncBack = sigmoid_backward\n",
    "    elif layer['actFunc'] is 'relu':\n",
    "        actFuncBack = relu_backward\n",
    "    else:\n",
    "        actFuncBack = tanh_backward\n",
    "\n",
    "    # 计算当前层外层导数\n",
    "    # dZ_now = actFunc'(Z_hide)\n",
    "    dZ_now = actFuncBack(dA_now, Z_now)\n",
    "    # dW_now = actFunc'(Z_hide) * (X=Z_hide*dW)\n",
    "#     print('X_pre',X_pre.shape)\n",
    "#     print('dZ_now',dZ_now.shape)\n",
    "#     print('dA_now',dA_now.shape)\n",
    "#     print('Z_now',Z_now.shape)\n",
    "    dW_now = X_pre.T.dot(dZ_now) / back_dim\n",
    "    # db_now = actFunc'(Z_hide) * (1=Z_hide*db); 维度转换\n",
    "    db_now = np.sum(dZ_now, axis=0, keepdims=True) / back_dim\n",
    "#     print('dW_now:',dW_now.shape)\n",
    "#     print('db_now',db_now.shape)\n",
    "    # dA_pre为向前一层传递的梯度；对上一层的Z_out即本层的X求导结果\n",
    "    # dA_pre = actFunc'(Z_hide) * (W=Z_hide*dX)\n",
    "    W_now = copy.deepcopy(layer['W'])\n",
    "    dA_pre = dZ_now.dot(W_now.T)\n",
    "#     print('dA_pre',dA_pre.shape)\n",
    "    \n",
    "    return dA_pre,dW_now, db_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_layers_backward_prop(Z_out, memory_forward, layers, X, y):\n",
    "#     Z_out, memo_forward = full_layers_forward_prop(X, layers)\n",
    "    # 反向传播\n",
    "    probs = softmax(Z_out)\n",
    "    probs[range(num_examples), y] -= 1\n",
    "    dA_pre = probs\n",
    "#     print('dA_now:', dA_now.shape)\n",
    "#     print('probs:', probs.shape)\n",
    "    memory_backward = []\n",
    "    layers.reverse()\n",
    "    memory_forward.reverse()\n",
    "\n",
    "    length = len(layers)\n",
    "    for idx in range(length):\n",
    "#         print('layer_now:', idx)\n",
    "        dA_pre, dW_now, db_now = single_layer_backward_prop(memory_forward[idx],memory_forward[idx+1],dA_pre,layers[idx])\n",
    "        memo_backward = {\n",
    "            'dW_now': dW_now,\n",
    "            'db_now': db_now\n",
    "        }\n",
    "        memory_backward.append(memo_backward)\n",
    "\n",
    "    return memory_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(layers, memory_backward):\n",
    "#     print('layers: ',len(layers)\n",
    "#     print('memory_backward: ', len(memory_backward))\n",
    "    length = len(layers)\n",
    "#     print(memory_backward)\n",
    "#     print(layers)\n",
    "#     print(memory_backward)\n",
    "    for idx in range(length):\n",
    "        dW = memory_backward[idx]['dW_now']\n",
    "#         print('dW.shape: ', dW.shape)\n",
    "        layers[idx]['W'] -= epsilon * memory_backward[idx]['dW_now']\n",
    "        layers[idx]['b'] -= epsilon * memory_backward[idx]['db_now']\n",
    "        \n",
    "#     print(memory_backward)\n",
    "#     print(layers)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, layers):\n",
    "    Z_out, memory_forward = full_layers_forward_prop(X,layers)\n",
    "    probs = softmax(Z_out)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(X, layers):\n",
    "    Z_out, memory_forward = full_layers_forward_prop(X,layers)\n",
    "    probs = softmax(Z_out)\n",
    "#     print(probs)\n",
    "#     return np.argmax(probs, axis=1)\n",
    "    acc = np.mean(Y_test==np.argmax(probs, axis=1))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, nn_architcture, epochs):\n",
    "    layers = init_layers(nn_architcture)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    best_acc = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        Z_out, memory_forward = full_layers_forward_prop(X,layers)\n",
    "#         print(Z_out.shape)\n",
    "        cost = loss(Z_out, y)\n",
    "        acc = get_acc(X_test, layers)\n",
    "        cost_history.append(cost)\n",
    "        accuracy_history.append(acc)\n",
    "        if best_acc < acc :\n",
    "            best_acc = acc\n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            print('||best_acc => ', best_acc, '||cost => ', cost, '||acc => ', acc)\n",
    "            print('cost: ', cost)\n",
    "            print('acc: ', acc)\n",
    "\n",
    "\n",
    "        memory_backward = full_layers_backward_prop(Z_out, memory_forward, layers, X, y)\n",
    "        layers = update(layers, memory_backward)\n",
    "        \n",
    "        layers.reverse()\n",
    "        memory_forward.reverse()\n",
    "\n",
    "    return layers, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数量： 2034\n",
      "输入样本维度： 20\n",
      "输出数量： 4\n"
     ]
    }
   ],
   "source": [
    "num_examples = X_train.shape[0] # 训练样本的数量\n",
    "nn_input_dim = X_train.shape[1] # 输入层的维度\n",
    "nn_output_dim = 4 # 输出层的维度\n",
    "\n",
    "# 梯度下降的参数（我直接手动赋值）\n",
    "epsilon = 0.1 # 梯度下降的学习率\n",
    "reg_lambda = 0.01 # 正则化的强度\n",
    "epochs = 500\n",
    "\n",
    "print('样本数量：', num_examples)\n",
    "print('输入样本维度：',nn_input_dim)\n",
    "print('输出数量：',nn_output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (20, 4)\n",
      "b: (1, 4)\n",
      "||best_acc =>  0.20916481892091648 ||cost =>  1.425108324743589 ||acc =>  0.20916481892091648\n",
      "cost:  1.425108324743589\n",
      "acc:  0.20916481892091648\n",
      "||best_acc =>  0.2727272727272727 ||cost =>  1.359792129615471 ||acc =>  0.2727272727272727\n",
      "cost:  1.359792129615471\n",
      "acc:  0.2727272727272727\n",
      "||best_acc =>  0.31337767923133775 ||cost =>  1.3462430321413155 ||acc =>  0.31337767923133775\n",
      "cost:  1.3462430321413155\n",
      "acc:  0.31337767923133775\n",
      "||best_acc =>  0.352549889135255 ||cost =>  1.3350477237431118 ||acc =>  0.352549889135255\n",
      "cost:  1.3350477237431118\n",
      "acc:  0.352549889135255\n",
      "||best_acc =>  0.3991130820399113 ||cost =>  1.3249253488879633 ||acc =>  0.3991130820399113\n",
      "cost:  1.3249253488879633\n",
      "acc:  0.3991130820399113\n",
      "||best_acc =>  0.42424242424242425 ||cost =>  1.315737045844439 ||acc =>  0.42424242424242425\n",
      "cost:  1.315737045844439\n",
      "acc:  0.42424242424242425\n",
      "||best_acc =>  0.43754619364375463 ||cost =>  1.3073874617082992 ||acc =>  0.43754619364375463\n",
      "cost:  1.3073874617082992\n",
      "acc:  0.43754619364375463\n",
      "||best_acc =>  0.44715447154471544 ||cost =>  1.299787807536305 ||acc =>  0.44715447154471544\n",
      "cost:  1.299787807536305\n",
      "acc:  0.44715447154471544\n",
      "||best_acc =>  0.45454545454545453 ||cost =>  1.292856235976506 ||acc =>  0.45454545454545453\n",
      "cost:  1.292856235976506\n",
      "acc:  0.45454545454545453\n",
      "||best_acc =>  0.4597191426459719 ||cost =>  1.2865187110436402 ||acc =>  0.458980044345898\n",
      "cost:  1.2865187110436402\n",
      "acc:  0.458980044345898\n",
      "Decision Boundary for hidden layer size 1. acc:  0.458980044345898\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "#     {'input_dim': nn_input_dim, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': nn_input_dim, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history = train(X_train, Y_train, nn_architcture, epochs)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 1.\")\n",
    "print(\"Decision Boundary for hidden layer size 1. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (20, 8)\n",
      "b: (1, 8)\n",
      "w: (8, 4)\n",
      "b: (1, 4)\n",
      "||best_acc =>  0.24611973392461198 ||cost =>  1.396465543343874 ||acc =>  0.24611973392461198\n",
      "cost:  1.396465543343874\n",
      "acc:  0.24611973392461198\n",
      "||best_acc =>  0.31929046563192903 ||cost =>  1.35794925102426 ||acc =>  0.31929046563192903\n",
      "cost:  1.35794925102426\n",
      "acc:  0.31929046563192903\n",
      "||best_acc =>  0.36954915003695493 ||cost =>  1.3400043177270022 ||acc =>  0.36954915003695493\n",
      "cost:  1.3400043177270022\n",
      "acc:  0.36954915003695493\n",
      "||best_acc =>  0.4042867701404287 ||cost =>  1.3225403500385093 ||acc =>  0.4042867701404287\n",
      "cost:  1.3225403500385093\n",
      "acc:  0.4042867701404287\n",
      "||best_acc =>  0.4205469327420547 ||cost =>  1.3049645718869975 ||acc =>  0.4198078344419808\n",
      "cost:  1.3049645718869975\n",
      "acc:  0.4198078344419808\n",
      "||best_acc =>  0.43385070214338506 ||cost =>  1.2876367747771897 ||acc =>  0.43163340724316335\n",
      "cost:  1.2876367747771897\n",
      "acc:  0.43163340724316335\n",
      "||best_acc =>  0.43754619364375463 ||cost =>  1.2712837947226325 ||acc =>  0.43754619364375463\n",
      "cost:  1.2712837947226325\n",
      "acc:  0.43754619364375463\n",
      "||best_acc =>  0.44715447154471544 ||cost =>  1.2565265724224397 ||acc =>  0.44715447154471544\n",
      "cost:  1.2565265724224397\n",
      "acc:  0.44715447154471544\n",
      "||best_acc =>  0.450849963045085 ||cost =>  1.2436611533366462 ||acc =>  0.44863266814486324\n",
      "cost:  1.2436611533366462\n",
      "acc:  0.44863266814486324\n",
      "||best_acc =>  0.4575018477457502 ||cost =>  1.2326812754104954 ||acc =>  0.4575018477457502\n",
      "cost:  1.2326812754104954\n",
      "acc:  0.4575018477457502\n",
      "Decision Boundary for hidden layer size 2, acc:  0.4656319290465632\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 8, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 8, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 2.\")\n",
    "print(\"Decision Boundary for hidden layer size 2, acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (20, 20)\n",
      "b: (1, 20)\n",
      "w: (20, 20)\n",
      "b: (1, 20)\n",
      "w: (20, 4)\n",
      "b: (1, 4)\n",
      "||best_acc =>  0.2402069475240207 ||cost =>  1.404561893587037 ||acc =>  0.2402069475240207\n",
      "cost:  1.404561893587037\n",
      "acc:  0.2402069475240207\n",
      "||best_acc =>  0.3821138211382114 ||cost =>  1.3391960117285997 ||acc =>  0.38137472283813745\n",
      "cost:  1.3391960117285997\n",
      "acc:  0.38137472283813745\n",
      "||best_acc =>  0.44567627494456763 ||cost =>  1.301085662828957 ||acc =>  0.4449371766444937\n",
      "cost:  1.301085662828957\n",
      "acc:  0.4449371766444937\n",
      "||best_acc =>  0.4575018477457502 ||cost =>  1.2677228345896416 ||acc =>  0.45528455284552843\n",
      "cost:  1.2677228345896416\n",
      "acc:  0.45528455284552843\n",
      "||best_acc =>  0.4575018477457502 ||cost =>  1.24138498344743 ||acc =>  0.4530672579453067\n",
      "cost:  1.24138498344743\n",
      "acc:  0.4530672579453067\n",
      "||best_acc =>  0.4575018477457502 ||cost =>  1.222188838623472 ||acc =>  0.4530672579453067\n",
      "cost:  1.222188838623472\n",
      "acc:  0.4530672579453067\n",
      "||best_acc =>  0.4575018477457502 ||cost =>  1.2085526612234936 ||acc =>  0.4515890613451589\n",
      "cost:  1.2085526612234936\n",
      "acc:  0.4515890613451589\n",
      "||best_acc =>  0.4619364375461936 ||cost =>  1.198698562049525 ||acc =>  0.4619364375461936\n",
      "cost:  1.198698562049525\n",
      "acc:  0.4619364375461936\n",
      "||best_acc =>  0.4648928307464893 ||cost =>  1.1913282325346892 ||acc =>  0.4626755358462675\n",
      "cost:  1.1913282325346892\n",
      "acc:  0.4626755358462675\n",
      "||best_acc =>  0.4663710273466371 ||cost =>  1.1856220451152475 ||acc =>  0.4663710273466371\n",
      "cost:  1.1856220451152475\n",
      "acc:  0.4663710273466371\n",
      "Decision Boundary for hidden layer size 3. acc:  0.4663710273466371\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 20, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 20, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 20, 'output_dim': 8, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 20, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 3\")\n",
    "print(\"Decision Boundary for hidden layer size 3. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (20, 4)\n",
      "b: (1, 4)\n",
      "w: (4, 6)\n",
      "b: (1, 6)\n",
      "w: (6, 4)\n",
      "b: (1, 4)\n",
      "w: (4, 4)\n",
      "b: (1, 4)\n",
      "||best_acc =>  0.19660014781966 ||cost =>  1.399085128895874 ||acc =>  0.19660014781966\n",
      "cost:  1.399085128895874\n",
      "acc:  0.19660014781966\n",
      "||best_acc =>  0.2838137472283814 ||cost =>  1.3723064931815914 ||acc =>  0.2838137472283814\n",
      "cost:  1.3723064931815914\n",
      "acc:  0.2838137472283814\n",
      "||best_acc =>  0.3155949741315595 ||cost =>  1.3652055561990348 ||acc =>  0.3155949741315595\n",
      "cost:  1.3652055561990348\n",
      "acc:  0.3155949741315595\n",
      "||best_acc =>  0.3399852180339985 ||cost =>  1.3465000461528684 ||acc =>  0.3399852180339985\n",
      "cost:  1.3465000461528684\n",
      "acc:  0.3399852180339985\n",
      "||best_acc =>  0.36881005173688103 ||cost =>  1.3188478635829892 ||acc =>  0.36881005173688103\n",
      "cost:  1.3188478635829892\n",
      "acc:  0.36881005173688103\n",
      "||best_acc =>  0.3917220990391722 ||cost =>  1.2891180941147171 ||acc =>  0.3909830007390983\n",
      "cost:  1.2891180941147171\n",
      "acc:  0.3909830007390983\n",
      "||best_acc =>  0.4072431633407243 ||cost =>  1.2647672587684138 ||acc =>  0.4072431633407243\n",
      "cost:  1.2647672587684138\n",
      "acc:  0.4072431633407243\n",
      "||best_acc =>  0.4190687361419069 ||cost =>  1.247915920600603 ||acc =>  0.4190687361419069\n",
      "cost:  1.247915920600603\n",
      "acc:  0.4190687361419069\n",
      "||best_acc =>  0.42276422764227645 ||cost =>  1.2367306475677673 ||acc =>  0.42276422764227645\n",
      "cost:  1.2367306475677673\n",
      "acc:  0.42276422764227645\n",
      "||best_acc =>  0.42645971914264597 ||cost =>  1.229034075641192 ||acc =>  0.41611234294161126\n",
      "cost:  1.229034075641192\n",
      "acc:  0.41611234294161126\n",
      "Decision Boundary for hidden layer size 4. acc:  0.4153732446415373\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 4\")\n",
    "print(\"Decision Boundary for hidden layer size 4. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (20, 4)\n",
      "b: (1, 4)\n",
      "w: (4, 6)\n",
      "b: (1, 6)\n",
      "w: (6, 6)\n",
      "b: (1, 6)\n",
      "w: (6, 4)\n",
      "b: (1, 4)\n",
      "w: (4, 4)\n",
      "b: (1, 4)\n",
      "||best_acc =>  0.21581670362158167 ||cost =>  1.3978359110802032 ||acc =>  0.21581670362158167\n",
      "cost:  1.3978359110802032\n",
      "acc:  0.21581670362158167\n",
      "||best_acc =>  0.2749445676274945 ||cost =>  1.3724994102252872 ||acc =>  0.2749445676274945\n",
      "cost:  1.3724994102252872\n",
      "acc:  0.2749445676274945\n",
      "||best_acc =>  0.2934220251293422 ||cost =>  1.3707427201207483 ||acc =>  0.2926829268292683\n",
      "cost:  1.3707427201207483\n",
      "acc:  0.2926829268292683\n",
      "||best_acc =>  0.30303030303030304 ||cost =>  1.3699802359861153 ||acc =>  0.30229120473022913\n",
      "cost:  1.3699802359861153\n",
      "acc:  0.30229120473022913\n",
      "||best_acc =>  0.3082039911308204 ||cost =>  1.3692390315854386 ||acc =>  0.3074648928307465\n",
      "cost:  1.3692390315854386\n",
      "acc:  0.3074648928307465\n",
      "||best_acc =>  0.3170731707317073 ||cost =>  1.3681552455414112 ||acc =>  0.3170731707317073\n",
      "cost:  1.3681552455414112\n",
      "acc:  0.3170731707317073\n",
      "||best_acc =>  0.3311160384331116 ||cost =>  1.3659535342356937 ||acc =>  0.3303769401330377\n",
      "cost:  1.3659535342356937\n",
      "acc:  0.3303769401330377\n",
      "||best_acc =>  0.3385070214338507 ||cost =>  1.3573722573358447 ||acc =>  0.3377679231337768\n",
      "cost:  1.3573722573358447\n",
      "acc:  0.3377679231337768\n",
      "||best_acc =>  0.3532889874353289 ||cost =>  1.3246072812502674 ||acc =>  0.3532889874353289\n",
      "cost:  1.3246072812502674\n",
      "acc:  0.3532889874353289\n",
      "||best_acc =>  0.3909830007390983 ||cost =>  1.2870533934848052 ||acc =>  0.3895048041389505\n",
      "cost:  1.2870533934848052\n",
      "acc:  0.3895048041389505\n",
      "Decision Boundary for hidden layer size 4. acc:  0.4131559497413156\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs)\n",
    "acc = get_acc(X_test, layers)\n",
    "# plot_decision_boundary(lambda x: predict(x, layers))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 4\")\n",
    "print(\"Decision Boundary for hidden layer size 4. acc: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
