{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 100) (2034,)\n",
      "(1353, 100) (1353,)\n"
     ]
    }
   ],
   "source": [
    "# %% 1\n",
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',  categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',  categories=categories)\n",
    "\n",
    "# pprint(newsgroups_train.data[0])\n",
    "\n",
    "num_train = len(newsgroups_train.data)\n",
    "num_test  = len(newsgroups_test.data)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "X = vectorizer.fit_transform( newsgroups_train.data + newsgroups_test.data )\n",
    "X_train = X[0:num_train, :]\n",
    "X_test = X[num_train:num_train+num_test,:]\n",
    "\n",
    "Y_train = newsgroups_train.target\n",
    "Y_test = newsgroups_test.target\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(input_dim, output_dim, actFunc):\n",
    "    np.random.seed(0)\n",
    "    W = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)\n",
    "    b = np.zeros((1,output_dim))\n",
    "#     print('w:',W.shape)\n",
    "#     print('b:',b.shape)\n",
    "    layer = {'W': W, 'b': b, 'actFunc': actFunc}\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture):\n",
    "    layers = []\n",
    "    for l in nn_architecture:\n",
    "        layer = init_layer(l['input_dim'], l['output_dim'], l['actFunc'])\n",
    "        layers.append(layer)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_scores = np.exp(Z)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1-sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def tanh_backward(dA, Z):\n",
    "    t = np.tanh(Z)\n",
    "    res = (1 - t * t)\n",
    "#     print('res:', res.shape)\n",
    "#     print('dA:', dA.shape)\n",
    "    return res * dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Z, y):\n",
    "    # 计算损失\n",
    "    probs = softmax(Z)\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    #在损失上加上正则项（可选）\n",
    "    # data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_prop(X, layer):\n",
    "    W = layer['W']\n",
    "#     print(W.shape)\n",
    "    Z = X.dot(layer['W']) + layer['b']\n",
    "    if layer['actFunc'] is 'relu':\n",
    "        actFunction = relu\n",
    "    elif layer['actFunc'] is 'sigmoid':\n",
    "        actFunction = sigmoid\n",
    "    else:\n",
    "        actFunction = np.tanh\n",
    "    return actFunction(Z), Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_layers_forward_prop(X, layers):\n",
    "    memory_forward = []\n",
    "    Z_out = X\n",
    "    memo_forward = {\n",
    "        'Z_out': X\n",
    "    }\n",
    "    memory_forward.append(memo_forward)\n",
    "    layers_now = 0\n",
    "    for layer in layers:\n",
    "#         print('forward layers_now:',layers_now)\n",
    "        Z_out, Z_hide = single_layer_forward_prop(Z_out, layer)\n",
    "        memo_forward = {\n",
    "            'Z_out': Z_out,\n",
    "            'Z_hide': Z_hide\n",
    "        }\n",
    "        memory_forward.append(memo_forward)\n",
    "        layers_now += 1\n",
    "\n",
    "    # 返回最终的Z_out => actFunc(Z=X*W + b)\n",
    "    # memory_forward记录每一层的Z_out=actFunc(Z_hide)和Z_hide=W*X+b\n",
    "#     print('Z_out: ',Z_out.shape)\n",
    "    return Z_out, memory_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_prop(memo_forward_now, memo_forward_pre, dA_now, layer):\n",
    "    # 前向神经元个数\n",
    "    # dA_now为由下一层传回的梯度\n",
    "    # memo_forward_pre 记录上一层计算结果， Z_hide=X*w+b和Z_out => X_pre\n",
    "    # memo_forward_now 记录当前层的计算结果，Z_hide => Z_now和Z_out\n",
    "    X_pre = memo_forward_pre['Z_out']\n",
    "    Z_now = memo_forward_now['Z_hide']\n",
    "    back_dim = X_pre.shape[0]\n",
    "\n",
    "    if layer['actFunc'] is 'sigmoid':\n",
    "        actFuncBack = sigmoid_backward\n",
    "    elif layer['actFunc'] is 'relu':\n",
    "        actFuncBack = relu_backward\n",
    "    else:\n",
    "        actFuncBack = tanh_backward\n",
    "\n",
    "    # 计算当前层外层导数\n",
    "    # dZ_now = actFunc'(Z_hide)\n",
    "    dZ_now = actFuncBack(dA_now, Z_now)\n",
    "    # dW_now = actFunc'(Z_hide) * (X=Z_hide*dW)\n",
    "#     print('X_pre',X_pre.shape)\n",
    "#     print('dZ_now',dZ_now.shape)\n",
    "#     print('dA_now',dA_now.shape)\n",
    "#     print('Z_now',Z_now.shape)\n",
    "    dW_now = X_pre.T.dot(dZ_now) / back_dim\n",
    "    # db_now = actFunc'(Z_hide) * (1=Z_hide*db); 维度转换\n",
    "    db_now = np.sum(dZ_now, axis=0, keepdims=True) / back_dim\n",
    "#     print('dW_now:',dW_now.shape)\n",
    "#     print('db_now',db_now.shape)\n",
    "    # dA_pre为向前一层传递的梯度；对上一层的Z_out即本层的X求导结果\n",
    "    # dA_pre = actFunc'(Z_hide) * (W=Z_hide*dX)\n",
    "    W_now = copy.deepcopy(layer['W'])\n",
    "    dA_pre = dZ_now.dot(W_now.T)\n",
    "#     print('dA_pre',dA_pre.shape)\n",
    "    \n",
    "    return dA_pre,dW_now, db_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_layers_backward_prop(Z_out, memory_forward, layers, X, y):\n",
    "#     Z_out, memo_forward = full_layers_forward_prop(X, layers)\n",
    "    # 反向传播\n",
    "    probs = softmax(Z_out)\n",
    "    probs[range(num_examples), y] -= 1\n",
    "    dA_pre = probs\n",
    "#     print('dA_now:', dA_now.shape)\n",
    "#     print('probs:', probs.shape)\n",
    "    memory_backward = []\n",
    "    layers.reverse()\n",
    "    memory_forward.reverse()\n",
    "\n",
    "    length = len(layers)\n",
    "    for idx in range(length):\n",
    "#         print('layer_now:', idx)\n",
    "        dA_pre, dW_now, db_now = single_layer_backward_prop(memory_forward[idx],memory_forward[idx+1],dA_pre,layers[idx])\n",
    "        memo_backward = {\n",
    "            'dW_now': dW_now,\n",
    "            'db_now': db_now\n",
    "        }\n",
    "        memory_backward.append(memo_backward)\n",
    "\n",
    "    return memory_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(layers, memory_backward,learning_rate):\n",
    "#     print('layers: ',len(layers)\n",
    "#     print('memory_backward: ', len(memory_backward))\n",
    "    length = len(layers)\n",
    "#     print(memory_backward)\n",
    "#     print(layers)\n",
    "#     print(memory_backward)\n",
    "    for idx in range(length):\n",
    "        dW = memory_backward[idx]['dW_now']\n",
    "#         print('dW.shape: ', dW.shape)\n",
    "        layers[idx]['W'] -= learning_rate * memory_backward[idx]['dW_now']\n",
    "        layers[idx]['b'] -= learning_rate * memory_backward[idx]['db_now']\n",
    "        \n",
    "#         sgd_momentum(w, dw, config=None)\n",
    "        \n",
    "        \n",
    "#     print(memory_backward)\n",
    "#     print(layers)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, layers):\n",
    "    Z_out, memory_forward = full_layers_forward_prop(X,layers)\n",
    "    probs = softmax(Z_out)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(X, layers):\n",
    "    acc = np.mean(Y_test==predict(X, layers))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train(X, y, nn_architcture, epochs,base_lr):\n",
    "    layers = init_layers(nn_architcture)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    best_acc = 0\n",
    "    lr=base_lr\n",
    "    \n",
    "#     gamma = 0.4\n",
    "    gamma = 0.99998\n",
    "    power = 0.9998\n",
    "    stepsize = 100\n",
    "    \n",
    "    # sigmoid\n",
    "    gamma = 0.05\n",
    "    stepsize = 200  \n",
    "    i_lr = 1\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Z_out, memory_forward = full_layers_forward_prop(X,layers)\n",
    "#         print(Z_out.shape)\n",
    "        cost = loss(Z_out, y)\n",
    "        acc = get_acc(X_test, layers)\n",
    "        cost_history.append(cost)\n",
    "        accuracy_history.append(acc)\n",
    "        if best_acc < acc :\n",
    "            best_acc = acc\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "#             print('||best_acc => ', best_acc, '||cost => ', cost, '||acc => ', acc)\n",
    "#             print('cost: ', cost)\n",
    "            print('acc: ', acc)\n",
    "            print('learning_rate',lr)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "#             lr=lr * gamma **( math.floor ( i_lr / stepsize ) )\n",
    "#               lr=lr * gamma ** i_lr\n",
    "#               lr=lr * ( 1 - i_lr / 20 ) ** ( power )\n",
    "#               lr=lr * ( 1/ ( 1 + np.exp( gamma * ( i_lr - stepsize ) )) )\n",
    "              lr=lr * (1 + gamma * i_lr) ** (- power)\n",
    "              i_lr +=1\n",
    "    \n",
    "    \n",
    "#             print('i_lr',i_lr)\n",
    "#             print('learning_rate',lr)\n",
    "            \n",
    "        memory_backward = full_layers_backward_prop(Z_out, memory_forward, layers, X, y)\n",
    "        layers = update(layers, memory_backward,lr)\n",
    "        \n",
    "        \n",
    "        layers.reverse()\n",
    "        memory_forward.reverse()\n",
    "\n",
    "    return layers, cost_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数量： 2034\n",
      "输入样本维度： 100\n",
      "输出数量： 4\n"
     ]
    }
   ],
   "source": [
    "num_examples, nn_input_dim = X_train.shape  # 训练样本的数量  输入层的维度\n",
    "nn_output_dim = 4 # 输出层的维度\n",
    "\n",
    "# 梯度下降的参数（我直接手动赋值）\n",
    "epsilon = 0.05 # 初始的学习率\n",
    "reg_lambda = 0.01 # 正则化的强度\n",
    "epochs = 2000\n",
    "\n",
    "print('样本数量：', num_examples)\n",
    "print('输入样本维度：',nn_input_dim)\n",
    "print('输出数量：',nn_output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nn_architcture = [\n",
    "#      {'input_dim': nn_input_dim, 'output_dim': 20, 'actFunc': 'relu'},\n",
    "#     {'input_dim': 20, 'output_dim': 20, 'actFunc': 'relu'},\n",
    "# #     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "# #     {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 20, 'output_dim': nn_output_dim, 'actFunc': 'relu'},\n",
    "# ]\n",
    "\n",
    "# layers, cost_history,accuracy_history = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "# acc = get_acc(X_test, layers)\n",
    "\n",
    "# print(\"hidden layer size 1. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.18551367331855137\n",
      "learning_rate 0.05\n",
      "acc:  0.3436807095343681\n",
      "learning_rate 0.04761951228954494\n",
      "acc:  0.4715447154471545\n",
      "learning_rate 0.04329129093004749\n",
      "acc:  0.5395417590539542\n",
      "learning_rate 0.03764565307994942\n",
      "acc:  0.5639320029563932\n",
      "learning_rate 0.031372521523160536\n",
      "acc:  0.5831485587583148\n",
      "learning_rate 0.02509913733566155\n",
      "acc:  0.5920177383592018\n",
      "learning_rate 0.01930804184119793\n",
      "acc:  0.5912786400591279\n",
      "learning_rate 0.014303111675839177\n",
      "acc:  0.5971914264597191\n",
      "learning_rate 0.010217195877300909\n",
      "acc:  0.5971914264597191\n",
      "learning_rate 0.007046865636579731\n",
      "acc:  0.6001478196600147\n",
      "learning_rate 0.00469829140758544\n",
      "acc:  0.6008869179600886\n",
      "learning_rate 0.0030314214422638725\n",
      "acc:  0.6001478196600147\n",
      "learning_rate 0.0018948165071707788\n",
      "acc:  0.6008869179600886\n",
      "learning_rate 0.0011484886618975552\n",
      "acc:  0.6008869179600886\n",
      "learning_rate 0.0006756532661595908\n",
      "acc:  0.6008869179600886\n",
      "learning_rate 0.0003861307952220661\n",
      "acc:  0.6008869179600886\n",
      "learning_rate 0.0002145423279981942\n",
      "acc:  0.6008869179600886\n",
      "learning_rate 0.00011598309529404238\n",
      "acc:  0.6008869179600886\n",
      "learning_rate 6.105157109990433e-05\n",
      "acc:  0.6016260162601627\n",
      "learning_rate 3.131268002614783e-05\n",
      "hidden layer size 3. acc:  0.6016260162601627\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 20, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 20, 'output_dim': 20, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 20, 'output_dim': 8, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 20, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "\n",
    "print(\"hidden layer size 3. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.29637841832963785\n",
      "learning_rate 0.05\n",
      "acc:  0.4138950480413895\n",
      "learning_rate 0.04761951228954494\n",
      "acc:  0.49223946784922396\n",
      "learning_rate 0.04329129093004749\n",
      "acc:  0.5343680709534369\n",
      "learning_rate 0.03764565307994942\n",
      "acc:  0.5535846267553585\n",
      "learning_rate 0.031372521523160536\n",
      "acc:  0.5691056910569106\n",
      "learning_rate 0.02509913733566155\n",
      "acc:  0.5794530672579453\n",
      "learning_rate 0.01930804184119793\n",
      "acc:  0.5853658536585366\n",
      "learning_rate 0.014303111675839177\n",
      "acc:  0.5868440502586844\n",
      "learning_rate 0.010217195877300909\n",
      "acc:  0.5868440502586844\n",
      "learning_rate 0.007046865636579731\n",
      "acc:  0.5898004434589801\n",
      "learning_rate 0.00469829140758544\n",
      "acc:  0.590539541759054\n",
      "learning_rate 0.0030314214422638725\n",
      "acc:  0.5920177383592018\n",
      "learning_rate 0.0018948165071707788\n",
      "acc:  0.5920177383592018\n",
      "learning_rate 0.0011484886618975552\n",
      "acc:  0.5927568366592757\n",
      "learning_rate 0.0006756532661595908\n",
      "acc:  0.5927568366592757\n",
      "learning_rate 0.0003861307952220661\n",
      "acc:  0.5927568366592757\n",
      "learning_rate 0.0002145423279981942\n",
      "acc:  0.5927568366592757\n",
      "learning_rate 0.00011598309529404238\n",
      "acc:  0.5927568366592757\n",
      "learning_rate 6.105157109990433e-05\n",
      "acc:  0.5927568366592757\n",
      "learning_rate 3.131268002614783e-05\n",
      "hidden layer size 2, acc:  0.5927568366592757\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 8, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 8, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "\n",
    "print(\"hidden layer size 2, acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.28085735402808576\n",
      "learning_rate 0.05\n",
      "acc:  0.42350332594235035\n",
      "learning_rate 0.04761951228954494\n",
      "acc:  0.49002217294900224\n",
      "learning_rate 0.04329129093004749\n",
      "acc:  0.5210643015521065\n",
      "learning_rate 0.03764565307994942\n",
      "acc:  0.5336289726533628\n",
      "learning_rate 0.031372521523160536\n",
      "acc:  0.5424981522542498\n",
      "learning_rate 0.02509913733566155\n",
      "acc:  0.5424981522542498\n",
      "learning_rate 0.01930804184119793\n",
      "acc:  0.5439763488543976\n",
      "learning_rate 0.014303111675839177\n",
      "acc:  0.5484109386548411\n",
      "learning_rate 0.010217195877300909\n",
      "acc:  0.5513673318551368\n",
      "learning_rate 0.007046865636579731\n",
      "acc:  0.5521064301552107\n",
      "learning_rate 0.00469829140758544\n",
      "acc:  0.5543237250554324\n",
      "learning_rate 0.0030314214422638725\n",
      "acc:  0.5558019216555802\n",
      "learning_rate 0.0018948165071707788\n",
      "acc:  0.5558019216555802\n",
      "learning_rate 0.0011484886618975552\n",
      "acc:  0.5565410199556541\n",
      "learning_rate 0.0006756532661595908\n",
      "acc:  0.5565410199556541\n",
      "learning_rate 0.0003861307952220661\n",
      "acc:  0.557280118255728\n",
      "learning_rate 0.0002145423279981942\n",
      "acc:  0.557280118255728\n",
      "learning_rate 0.00011598309529404238\n",
      "acc:  0.557280118255728\n",
      "learning_rate 6.105157109990433e-05\n",
      "acc:  0.557280118255728\n",
      "learning_rate 3.131268002614783e-05\n",
      "hidden layer size 4. acc:  0.557280118255728\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "#     {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  = train(X_train, Y_train, nn_architcture, epochs,epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "\n",
    "print(\"hidden layer size 4. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.19807834441980784\n",
      "learning_rate 0.05\n",
      "acc:  0.3296378418329638\n",
      "learning_rate 0.04761951228954494\n",
      "acc:  0.43089430894308944\n",
      "learning_rate 0.04329129093004749\n",
      "acc:  0.5040650406504065\n",
      "learning_rate 0.03764565307994942\n",
      "acc:  0.5262379896526238\n",
      "learning_rate 0.031372521523160536\n",
      "acc:  0.5388026607538803\n",
      "learning_rate 0.02509913733566155\n",
      "acc:  0.549150036954915\n",
      "learning_rate 0.01930804184119793\n",
      "acc:  0.5506282335550629\n",
      "learning_rate 0.014303111675839177\n",
      "acc:  0.5528455284552846\n",
      "learning_rate 0.010217195877300909\n",
      "acc:  0.5543237250554324\n",
      "learning_rate 0.007046865636579731\n",
      "acc:  0.5558019216555802\n",
      "learning_rate 0.00469829140758544\n",
      "acc:  0.5580192165558019\n",
      "learning_rate 0.0030314214422638725\n",
      "acc:  0.5587583148558758\n",
      "learning_rate 0.0018948165071707788\n",
      "acc:  0.5580192165558019\n",
      "learning_rate 0.0011484886618975552\n",
      "acc:  0.5587583148558758\n",
      "learning_rate 0.0006756532661595908\n",
      "acc:  0.5580192165558019\n",
      "learning_rate 0.0003861307952220661\n",
      "acc:  0.5580192165558019\n",
      "learning_rate 0.0002145423279981942\n",
      "acc:  0.5580192165558019\n",
      "learning_rate 0.00011598309529404238\n",
      "acc:  0.5580192165558019\n",
      "learning_rate 6.105157109990433e-05\n",
      "acc:  0.5580192165558019\n",
      "learning_rate 3.131268002614783e-05\n",
      "hidden layer size 4. acc:  0.5580192165558019\n"
     ]
    }
   ],
   "source": [
    "nn_architcture = [\n",
    "    {'input_dim': nn_input_dim, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 6, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 6, 'output_dim': 4, 'actFunc': 'tanh'},\n",
    "    {'input_dim': 4, 'output_dim': nn_output_dim, 'actFunc': 'tanh'},\n",
    "]\n",
    "\n",
    "layers, cost_history,accuracy_history  =  train(X_train, Y_train, nn_architcture, epochs, epsilon)\n",
    "acc = get_acc(X_test, layers)\n",
    "\n",
    "print(\"hidden layer size 4. acc: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    带动量的sgd实现\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    learning_rate=config.setdefault('learning_rate', 1e-2)\n",
    "    mu=config.setdefault('momentum', 0.9)\n",
    "    v = config.get('velocity', np.zeros_like(w))\n",
    " \n",
    "    next_w = None \n",
    "    v = mu * v - learning_rate * dw \n",
    "    next_w = w + v\n",
    "    \n",
    "    print(learning_rate)\n",
    "\n",
    "    config['velocity'] = v\n",
    " \n",
    "    return next_w, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.11989999999999999\n",
      "{'learning_rate': 0.01, 'momentum': 0.9, 'velocity': -0.0001}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = 0.12\n",
    "dw = 0.01\n",
    "config={}\n",
    "next_w,cofig=sgd_momentum(w, dw, config)\n",
    "print(next_w)\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
