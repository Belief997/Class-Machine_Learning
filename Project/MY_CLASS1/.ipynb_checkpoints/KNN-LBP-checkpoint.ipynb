{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "#coding=utf-8\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data -- a 10000x3072 numpy array of uint8s. Each row(行) of the array stores a 32x32 colour image. \n",
    "#The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. 3072 = 3 * (32 * 32) RGB*VH\n",
    "#The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "#labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f,encoding='latin1') # 编码方式\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "    return X, Y\n",
    "def load_CIFAR10(dir):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(dir, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)    \n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    # load test dataset\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join('datasets', dir, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(X_test, X_train):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "    X_test -- 由numpy数组表示的测试集，大小为(图片长度 * 图片高度 * 3 , 测试样本数)\n",
    "    X_train -- 由numpy数组表示的训练集，大小为(图片长度 * 图片高度 * 3 , 训练样本数)\n",
    "    输出:\n",
    "    distances -- 测试数据与各个训练数据之间的距离,大小为(测试样本数, 训练样本数量)的numpy数组\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[1]\n",
    "    num_train = X_train.shape[1]\n",
    "    distances = np.zeros((num_test, num_train))\n",
    "    # (X_test - X_train)*(X_test - X_train) = -2X_test*X_train + X_test*X_test + X_train*X_train\n",
    "    dist1 = np.multiply(np.dot(X_test.T, X_train), -2)    # -2X_test*X_train, shape (num_test, num_train)\n",
    "    dist2 = np.sum(np.square(X_test.T), axis=1, keepdims=True)    # X_test*X_test, shape (num_test, 1)    元素方   1000*1\n",
    "    dist3 = np.sum(np.square(X_train), axis=0,keepdims=True)    # X_train*X_train, shape(1, num_train)   1 * 50000   每张图内求和\n",
    "    distances = np.sqrt(dist1 + dist2 + dist3)  # 不同维度的矩阵相加会扩展\n",
    "\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, X_train, Y_train, k = 1):\n",
    "    \"\"\" \n",
    "    输入:\n",
    "    X_test -- 由numpy数组表示的测试集，大小为(图片长度 * 图片高度 * 3 , 测试样本数)\n",
    "    X_train -- 由numpy数组表示的训练集，大小为(图片长度 * 图片高度 * 3 , 训练样本数)\n",
    "    Y_train -- 由numpy数组（向量）表示的训练标签，大小为 (1, 训练样本数)\n",
    "    k -- 选取与训练集最近邻的数量\n",
    "    输出:\n",
    "    Y_prediction -- 包含X_test中所有预测值的numpy数组（向量）\n",
    "    distances -- 由numpy数组表示的测试数据与各个训练数据之间的距离,大小为(测试样本数, 训练样本数)\n",
    "    \"\"\"\n",
    "    distances = distance(X_test, X_train) # 1000 * 50000\n",
    "    num_test = X_test.shape[1]            # 1000\n",
    "    Y_prediction = np.zeros(num_test)\n",
    "    for i in range(num_test):\n",
    "        dists_min_k = np.argsort(distances[i])[:k]           # 按照距离递增次序进行排序,选取距离最小的k个点 \n",
    "        y_labels_k = Y_train[0,dists_min_k]                  # 确定前k个点的所在类别\n",
    "        Y_prediction[i] = np.argmax(np.bincount(y_labels_k)) # 返回前k个点中出现频率最高的类别作为测试数据的预测分类\n",
    "\n",
    "    return Y_prediction, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_test, Y_test, X_train, Y_train, k = 1, print_correct = False):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "    X_test -- 由numpy数组表示的测试集，大小为(图片长度 * 图片高度 * 3 , 测试样本数)\n",
    "    X_train -- 由numpy数组表示的训练集，大小为(图片长度 * 图片高度 * 3 , 训练样本数)\n",
    "    Y_train -- 由numpy数组（向量）表示的训练标签，大小为 (1, 训练样本数)\n",
    "    Y_test -- 由numpy数组（向量）表示的测试标签，大小为 (1, 测试样本数)\n",
    "    k -- 选取与训练集最近邻的数量\n",
    "    print_correct -- 设置为true时，打印正确率\n",
    "    输出：\n",
    "    d -- 包含模型信息的字典\n",
    "    \"\"\"\n",
    "    Y_prediction, distances = predict(X_test, X_train, Y_train, k)\n",
    "    num_correct = np.sum(Y_prediction == Y_test)\n",
    "    accuracy = np.mean(Y_prediction == Y_test) # mean 取均值\n",
    "    if print_correct:\n",
    "        print('Correct %d/%d: The test accuracy: %f' % (num_correct, X_test.shape[1], accuracy))\n",
    "    d = {\"k\": k,\n",
    "         \"Y_prediction\": Y_prediction, \n",
    "         \"distances\" : distances,\n",
    "         \"accuracy\": accuracy}\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature as skft\n",
    "def Lbp(img):\n",
    "    # settings for LBP\n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "    lbp = skft.local_binary_pattern(img, n_points, radius, 'default') # ‘default’, ‘ror’, ‘uniform’, ‘var’\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp, normed=True, bins=n_bins, range=(0, n_bins))\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# list\\nclasses = ['plane', 'car', 'bird', 'cat', 'dear', 'dog', 'frog', 'horse', 'ship', 'truck']\\nnum_classes = len(classes)\\nnum_each_class = 7\\n\\nfor y, cls in enumerate(classes):\\n    idxs = np.flatnonzero(y_train == y)\\n    idxs = np.random.choice(idxs, num_each_class, replace=False)\\n    for i, idx in enumerate(idxs):\\n        plt_idx = i * num_classes + (y + 1)\\n        plt.subplot(num_each_class, num_classes, plt_idx)\\n        plt.imshow(X_train[idx].astype('uint8'))\\n        plt.axis('off')\\n        if i == 0:\\n            plt.title(cls)\\nplt.show()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'E:\\Dian\\Class\\Junior_spring\\ML\\cifar-10-python\\cifar-10-batches-py'\n",
    "\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# 训练样本，训练标签，测试集，测试标签\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# list\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'dear', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "num_each_class = 7\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, num_each_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + (y + 1)\n",
    "        plt.subplot(num_each_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\other_software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from skimage.color import rgb2gray\n",
    "\n",
    "size_train = 50000\n",
    "size_test = 1000\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1)).T #( ,50000) \n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1)).T    #( ,10000) \n",
    "Y_set_train = y_train[:size_train].reshape(1,-1)   # (1, )\n",
    "Y_set_test = y_test[:size_test].reshape(1,-1)   # (1, )\n",
    "\n",
    "\n",
    "X_set_train = X_train[:,:size_train]\n",
    "X_set_test = X_test[:,:size_test]\n",
    "\n",
    "del X_train, y_train\n",
    "del X_test, y_test\n",
    "\n",
    "#  转灰度图 ， 做 lbp\n",
    "X_Train_tmp = [] # list  \n",
    "X_Test_tmp = []\n",
    "test_tmp = []\n",
    "\n",
    "#print (\"size %d %d\" %(X_set_train.shape[0], X_set_train.shape[1]))#\n",
    "#print (\"size %d \" %(X_set_train.T[0].shape[0]))#\n",
    "for i in range(size_train):\n",
    "    tmp = []\n",
    "    tmp = Lbp(rgb2gray(X_set_train.T[i].reshape(32, 32,3) )).T\n",
    "    X_Train_tmp.append(tmp)\n",
    "for i in range(size_test):\n",
    "    tmp = []\n",
    "    tmp = Lbp(rgb2gray(X_set_test.T[i].reshape(32, 32,3) )).T\n",
    "    X_Test_tmp.append(tmp)\n",
    "    \n",
    "del X_set_test,X_set_train\n",
    "\n",
    "X_set_train = []\n",
    "X_set_test = []\n",
    "X_set_train = np.array(X_Train_tmp).T\n",
    "X_set_test = np.array(X_Test_tmp).T\n",
    "print (X_set_train.size)\n",
    "print (\">>size %d \" %(X_set_train.shape[0]))#\n",
    "print (X_set_test.size)\n",
    "print (\">>size %d \" %(X_set_test.shape[0]))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {}\n",
    "k = []\n",
    "accuracys = []\n",
    "# 1~10\n",
    "for i in range(1,11):\n",
    "    models[str(i)] = model(X_set_test, Y_set_test, X_set_train, Y_set_train, i, print_correct = False)\n",
    "    k.append(models[str(i)][\"k\"])\n",
    "    accuracys.append(models[str(i)][\"accuracy\"])\n",
    "plt.plot(k, accuracys)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('k')\n",
    "plt.show()\n",
    "\n",
    "del models,k,accuracys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"a = np.array([[1,2,3]])\n",
    "b = a.reshape(3,1)\n",
    "\n",
    "\n",
    "np.dot(b,a)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
